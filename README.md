# ChatGPT-Benchmark-on-Radiation-Oncology

Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: 
Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology

## ACR TXIT exam

This repository contains the American College of Radiation (ACR) radiation oncology in-training (TXIT) exam with annotations whether ChatGPT-3.5 and ChatGPT-4 answer the questions incorrectly (red 0: false answer by ChatGPT-3.5; blue 0: false answer by ChatGPT-4). The Excel file contails all the reponses from both versions of ChatGPT. Each sheet contains 30 questions. In the last two sheets, the distributions of correct answers based on certain topics are analysed.

The [exam sheet](https://www.acr.org/-/media/ACR/Files/DXIT-TXIT/ACR-2021-TXIT-Exam---Assembled.pdf) and the [knowledge domain definition](https://www.acr.org/-/media/ACR/Files/DXIT-TXIT/ACR-TXIT---Table-of-Specifications.pdf) are from the ACR official website.

## Red Journal Gray Zone Cases
This repository contains the [2022 red journal collection of gray zone cases](https://www.redjournal.org/content/grayzone) as well as ChatGPT-4's responses. For each case, ChatGPT-4's recommendations for treating the patient are attached at the end of the corresponding pdf file.
In addition, the files we used for the blind clinician vote are also displayed.
